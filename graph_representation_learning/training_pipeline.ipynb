{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.data\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "SECRETS = dotenv_values(\"../envs/graphs.env\")\n",
    "\n",
    "TRAIN_DATA_PATH = SECRETS['TRAIN_DATA_PATH']\n",
    "TEST_DATA_PATH = SECRETS['TEST_DATA_PATH']\n",
    "PULL_UP_STRENGTH, PULL_DOWN_STRENGTH = list(map(int, SECRETS['BETA'].split(':')))\n",
    "NUM_SAMPLES = int(SECRETS['NUM_SAMPLES'])\n",
    "MAX_LENGTH = int(SECRETS['MAX_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [ (np.load(TRAIN_DATA_PATH + f\"/train_data/circuit{i}.npz\")['adj_list'],  np.load(TRAIN_DATA_PATH + f\"/train_data/circuit{i}.npz\")['feature_matrix']) for i in range(1, 1000+1) ]\n",
    "\n",
    "zero_added_train_data = [(train_data[i][0], np.insert(train_data[i][1], -1, np.zeros((1, 1)), axis=1)) for i in range(len(train_data))]\n",
    "# np.expand_dims(train_data[0][1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/train/finalSim.xlsx\")\n",
    "df['circuit'] = df['circuit'].str.replace('tdlaycircuit', '').astype(int)\n",
    "df['tdlay'] = df['tdlay'].str.replace('p', '').astype(float)/1000\n",
    "df.set_index('circuit', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_added_train_data[0][1][:, -2] +  list(df['tdlay'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(zero_added_train_data)):\n",
    "    zero_added_train_data[i][1][:, -2] += list(df['tdlay'])[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_ds = [(\n",
    "#     zero_added_train_data[i][0], \n",
    "#     np.concatenate((zero_added_train_data[i][1][:, :-1],np.random.rand(zero_added_train_data[i][1].shape[0], 4)), axis = 1)\n",
    "    \n",
    "#     )]\n",
    "second_ds = []\n",
    "\n",
    "for i in range(len(zero_added_train_data)):\n",
    "    adj_list = zero_added_train_data[i][0]\n",
    "    feature_matrix = np.concatenate((zero_added_train_data[i][1][:, :-1],np.random.rand(zero_added_train_data[i][1].shape[0], 4)), axis = 1)\n",
    "    if feature_matrix.shape[0] != zero_added_train_data[i][1].shape[0]:\n",
    "        print(i, feature_matrix.shape, zero_added_train_data[i][1].shape)\n",
    "    second_ds.append((adj_list, feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "temp = dgl.graph((zero_added_train_data[i][0][0], zero_added_train_data[i][0][1]))\n",
    "temp = dgl.add_self_loop(temp)\n",
    "temp.ndata['x'] = torch.tensor(zero_added_train_data[i][1], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "temp1 = dgl.graph((second_ds[i][0][0], second_ds[i][0][1]))\n",
    "temp1 = dgl.add_self_loop(temp1)\n",
    "temp1.ndata['x'] = torch.tensor(second_ds[i][1])\n",
    "temp1.ndata['y'] = torch.tensor(zero_added_train_data[i][1][:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_classification_dataset, node_classification_dataset = [], []\n",
    "for i in range(len(zero_added_train_data)):\n",
    "    temp = dgl.graph((zero_added_train_data[i][0][0], zero_added_train_data[i][0][1]))\n",
    "    temp = dgl.add_self_loop(temp)\n",
    "    temp.ndata['x'] = torch.tensor(zero_added_train_data[i][1], dtype=torch.float32)\n",
    "    graph_classification_dataset.append(temp)\n",
    "\n",
    "for i in range(len(second_ds)):\n",
    "    temp1 = dgl.graph((second_ds[i][0][0], second_ds[i][0][1]))\n",
    "    temp1 = dgl.add_self_loop(temp1)\n",
    "    temp1.ndata['x'] = torch.tensor(second_ds[i][1], dtype=torch.float32)\n",
    "    temp1.ndata['y'] = torch.tensor(zero_added_train_data[i][1][:, -1], dtype=torch.float32)\n",
    "    temp1.ndata['y'] = torch.unsqueeze(temp1.ndata['y'], -1)\n",
    "    node_classification_dataset.append(temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  2.0000, 22.0000,  0.2090,  2.0000],\n",
       "        [ 1.0000,  1.0000,  2.0000, 22.0000,  0.2090,  8.0000],\n",
       "        [ 2.0000,  1.0000,  2.0000, 22.0000,  0.2090,  4.0000],\n",
       "        [ 3.0000,  2.0000,  2.0000, 22.0000,  0.2090, 29.0000],\n",
       "        [ 4.0000,  3.0000,  2.0000, 22.0000,  0.2090, 39.0000],\n",
       "        [ 5.0000,  4.0000,  2.0000, 22.0000,  0.2090, 47.0000],\n",
       "        [ 6.0000,  2.0000,  2.0000, 22.0000,  0.2090, 35.0000],\n",
       "        [ 7.0000,  0.0000,  2.0000, 22.0000,  0.2090, 11.0000],\n",
       "        [ 8.0000,  3.0000,  2.0000, 22.0000,  0.2090,  3.0000],\n",
       "        [ 9.0000,  2.0000,  2.0000, 22.0000,  0.2090, 41.0000],\n",
       "        [10.0000,  2.0000,  2.0000, 22.0000,  0.2090, 34.0000],\n",
       "        [11.0000,  2.0000,  2.0000, 22.0000,  0.2090,  2.0000],\n",
       "        [12.0000,  1.0000,  2.0000, 22.0000,  0.2090, 39.0000],\n",
       "        [13.0000,  1.0000,  2.0000, 22.0000,  0.2090, 25.0000],\n",
       "        [14.0000,  2.0000,  2.0000, 22.0000,  0.2090, 46.0000],\n",
       "        [15.0000,  1.0000,  2.0000, 22.0000,  0.2090, 23.0000],\n",
       "        [16.0000,  4.0000,  2.0000, 22.0000,  0.2090,  3.0000],\n",
       "        [17.0000,  0.0000,  2.0000, 22.0000,  0.2090, 16.0000],\n",
       "        [18.0000,  3.0000,  2.0000, 22.0000,  0.2090, 44.0000],\n",
       "        [19.0000,  2.0000,  2.0000, 22.0000,  0.2090,  3.0000],\n",
       "        [20.0000,  2.0000,  2.0000, 22.0000,  0.2090,  9.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_classification_dataset[0].ndata['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE_SPECIFIC_FEATURE_SIZE = 2\n",
    "# GRAPH_SPECIFIC_FEATURE_SIZE = 3\n",
    "# K_SIZE = 1\n",
    "# NOISE_SIZE = 4\n",
    "# DATASET_SIZE = 1000\n",
    "\n",
    "# def make_random_graph():\n",
    "#     num_nodes = np.random.randint(10, 40)\n",
    "#     src_nodes = np.array([i for i in range(num_nodes-1)])\n",
    "#     dst_nodes = np.array([i for i in range(1, num_nodes)])\n",
    "#     random_node_features = np.random.randint(0, 6, (num_nodes, NODE_SPECIFIC_FEATURE_SIZE))\n",
    "#     random_graph_features = np.random.randint(1, 50, (num_nodes, GRAPH_SPECIFIC_FEATURE_SIZE-1))    \n",
    "#     random_delay_features = np.random.rand(num_nodes,1)\n",
    "#     random_K_features = np.random.randint(1, 50, (num_nodes, 1))\n",
    "#     random_noise_features = np.random.rand(num_nodes, NOISE_SIZE)\n",
    "#     merged_true_features = np.concatenate((random_node_features, random_graph_features, random_delay_features, random_K_features), axis=1)\n",
    "\n",
    "#     \"\"\"\n",
    "#     [gate_number, type_of_gate, overall_input_cap, overall_output_cap, sizing_of_gate]\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     merged_false_features = np.concatenate((random_node_features, random_graph_features, random_delay_features, random_noise_features), axis=1)\n",
    "#     true_graph = dgl.graph((src_nodes, dst_nodes))\n",
    "#     true_graph = dgl.add_self_loop(true_graph)\n",
    "#     false_graph = dgl.graph((src_nodes, dst_nodes))\n",
    "#     false_graph = dgl.add_self_loop(false_graph)\n",
    "#     true_graph.ndata['x'] = torch.tensor(merged_true_features).float()\n",
    "#     false_graph.ndata['x'] = torch.tensor(merged_false_features).float()\n",
    "#     false_graph.ndata['y'] = torch.tensor(random_K_features).float()\n",
    "#     return true_graph, false_graph\n",
    "\n",
    "\n",
    "# make_random_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_classification_dataset = []\n",
    "# node_classification_dataset = []\n",
    "# for i in range(DATASET_SIZE):\n",
    "#     true_graph, false_graph = make_random_graph()\n",
    "#     graph_classification_dataset.append(true_graph)\n",
    "#     node_classification_dataset.append(false_graph)\n",
    "\n",
    "\n",
    "\n",
    "class GraphClassificationDataset(DGLDataset):\n",
    "    def __init__(self, dataset:list, labels:list):\n",
    "        super().__init__(name='graph_classification_dataset')\n",
    "        self.graphs = dataset\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "true_ds = GraphClassificationDataset(graph_classification_dataset, [1 for i in range(len(graph_classification_dataset))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassificationDataset(DGLDataset):\n",
    "    def __init__(self, dataset:list):\n",
    "        super().__init__(name='node_classification_dataset')\n",
    "        self.graphs = dataset\n",
    "    \n",
    "    def process(self):\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "node_ds = NodeClassificationDataset(node_classification_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassificationModel(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(NodeClassificationModel, self).__init__()        \n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "g = node_ds[0]\n",
    "model = NodeClassificationModel(g.ndata['x'].shape[1], 10, g.ndata['y'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k_values = [model(g, g.ndata['x']) for g in node_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dataset(dataset : GraphClassificationDataset , new_k_values: list):\n",
    "    new_graphs, new_labels = [], []\n",
    "    for i in range(len(new_k_values)):\n",
    "        new_graph = dgl.graph((dataset[i][0].edges()[0], dataset[i][0].edges()[1]))\n",
    "        new_graph.ndata['x'] = torch.cat((dataset[i][0].ndata['x'][:, :-1], new_k_values[i] ), dim = -1)\n",
    "        new_graphs.append(new_graph)\n",
    "        new_labels.append(1-dataset[i][1])\n",
    "\n",
    "    new_labels = torch.tensor(new_labels)\n",
    "\n",
    "    new_ds = GraphClassificationDataset(dataset.graphs, torch.tensor(new_labels))\n",
    "    \n",
    "\n",
    "    return new_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassificationModel(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GraphClassificationModel, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return F.sigmoid(dgl.mean_nodes(g, \"h\"))\n",
    "    \n",
    "\n",
    "graph_model = GraphClassificationModel(true_ds[0][0].ndata['x'].shape[1], 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9998]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_model(true_ds[0][0], true_ds[0][0].ndata['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dgl.dataloading import GraphDataLoader\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# num_examples = 1000\n",
    "# num_train = int(num_examples * 0.8)\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "# test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "# # train_dataloader = GraphDataLoader(\n",
    "# #     dataset, sampler=train_sampler, batch_size=5, drop_last=False\n",
    "# # )\n",
    "# # test_dataloader = GraphDataLoader(\n",
    "# #     dataset, sampler=test_sampler, batch_size=5, drop_last=False\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = NodeClassificationModel(g.ndata['x'].shape[1], 100, g.ndata['y'].shape[1]) # object of class `Generator` to train c-GAN\n",
    "discriminator = GraphClassificationModel(true_ds[0][0].ndata['x'].shape[1], 100, 1) # object of class `Discriminator` to train c-GAN\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary crossentropy loss to perform adversarial training of GAN\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=0.008)  # optimizer of `generator` object\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.002)  # optimizer of `discriminator` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adith\\AppData\\Local\\Temp\\ipykernel_15044\\847611993.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_ds = GraphClassificationDataset(dataset.graphs, torch.tensor(new_labels))\n",
      "C:\\Users\\adith\\AppData\\Local\\Temp\\ipykernel_15044\\711672344.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/7], Gen Loss: 4801.3989, Disc Loss: 3042.4062\n",
      "Epoch [1/7], Gen Loss: 3552.1196, Disc Loss: 2407.9399\n",
      "Epoch [2/7], Gen Loss: 2367.9373, Disc Loss: 1796.8173\n",
      "Epoch [3/7], Gen Loss: 1345.5211, Disc Loss: 1244.6881\n",
      "Epoch [4/7], Gen Loss: 644.1614, Disc Loss: 844.0099\n",
      "Epoch [5/7], Gen Loss: 307.2730, Disc Loss: 731.9952\n",
      "Epoch [6/7], Gen Loss: 180.4167, Disc Loss: 884.6262\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7 # Total number of training epochs\n",
    "discriminator.train()\n",
    "generator.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # for i in range(len(node_ds)):\n",
    "    discriminator.zero_grad()\n",
    "    real_loss = 0\n",
    "    generated_sizings = []\n",
    "    for i in range(len(true_ds)):\n",
    "        real_decision = discriminator(true_ds[i][0], true_ds[i][0].ndata['x'])\n",
    "        real_loss += criterion(real_decision, torch.ones_like(real_decision))\n",
    "\n",
    "        predicted_k = generator(node_ds[i], node_ds[i].ndata['x'])\n",
    "        predicted_k = predicted_k.clone().detach()\n",
    "        generated_sizings.append( predicted_k )\n",
    "    generated_dataset = modify_dataset(true_ds, generated_sizings)\n",
    "    \n",
    "    fake_loss = 0\n",
    "    for i in range(len(generated_dataset)):\n",
    "        fake_decision = discriminator(generated_dataset[i][0], generated_dataset[i][0].ndata['x'])\n",
    "        fake_loss += criterion(fake_decision, torch.zeros_like(fake_decision))\n",
    "\n",
    "    disc_loss = (real_loss + fake_loss) / 2  \n",
    "    disc_loss.backward()  \n",
    "    disc_optimizer.step() \n",
    "\n",
    "    generator.zero_grad()\n",
    "    gen_loss = 0\n",
    "    for i in range(len(generated_dataset)):\n",
    "        fake_decision = discriminator(generated_dataset[i][0], generated_dataset[i][0].ndata['x'])\n",
    "        gen_loss += criterion(fake_decision, torch.ones_like(fake_decision))\n",
    "    \n",
    "    gen_loss.backward()\n",
    "    gen_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
